{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73211d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "# Read original DICOM or original NRRD (LPS)\n",
    "img = sitk.ReadImage(r\"J:\\startup\\skull_ct.nrrd\")\n",
    "\n",
    "# Get original metadata\n",
    "size = img.GetSize()\n",
    "spacing = img.GetSpacing()\n",
    "origin = np.array(img.GetOrigin())\n",
    "direction = np.array(img.GetDirection()).reshape(3, 3)\n",
    "\n",
    "# LPS -> RAS conversion matrix\n",
    "lps_to_ras = np.diag([-1, -1, 1])\n",
    "\n",
    "# Apply direction fix\n",
    "new_direction = lps_to_ras @ direction\n",
    "img.SetDirection(new_direction.flatten())\n",
    "\n",
    "# Apply origin fix\n",
    "new_origin = lps_to_ras @ origin\n",
    "img.SetOrigin(tuple(new_origin))\n",
    "\n",
    "# DO NOT flip the image data again\n",
    "sitk.WriteImage(img, r\"J:\\startup\\skull_ct_RAS00.nrrd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "153c530d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.9999999999999999, 0.0, 0.0, 0.0, -0.9999999999999999, 0.0, 0.0, 0.0, 1.0)\n",
      "(119.8255078125, 249.2535078125, -823.18)\n"
     ]
    }
   ],
   "source": [
    "print(img.GetDirection())\n",
    "print(img.GetOrigin())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86cf0302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- System Configuration ---\n",
      "✅ GPU Detected: NVIDIA GeForce RTX 2050\n",
      "   CUDA Version: 12.1\n",
      "   Device Count: 1\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import nrrd\n",
    "from collections import OrderedDict\n",
    "\n",
    "# MONAI imports\n",
    "from monai.bundle import ConfigParser\n",
    "from monai.data import decollate_batch, list_data_collate\n",
    "from monai.utils import convert_to_dst_type, MetaKeys\n",
    "from torch.cuda.amp import autocast\n",
    "from monai.inferers import SlidingWindowInfererAdapt\n",
    "from monai.transforms import (\n",
    "    Compose, CropForegroundd, EnsureTyped, Invertd, KeepLargestConnectedComponentd,\n",
    "    Lambdad, LoadImaged, NormalizeIntensityd, Resized, ScaleIntensityRanged,\n",
    "    Spacingd, Orientationd, ConcatItemsd,\n",
    ")\n",
    "\n",
    "# --- System Check ---\n",
    "print(\"--- System Configuration ---\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"✅ GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   Device Count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠️  No GPU detected. Running on CPU (Inference will be slower).\")\n",
    "print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "455485d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Using raw strings (r\"...\") to handle Windows backslashes correctly\n",
    "MODEL_FILE = r\"J:\\startup\\whole-head-05mm-v1.0.1\\model.pt\"\n",
    "IMAGE_FILE = r\"J:\\startup\\skull_ct_RAS00.nrrd\"\n",
    "RESULT_FILE = r\"J:\\startup\\output_seg02.nrrd\"\n",
    "\n",
    "# Optional additional images (set to None if not used)\n",
    "IMAGE_FILE_2 = None\n",
    "IMAGE_FILE_3 = None\n",
    "IMAGE_FILE_4 = None\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def logits2pred(logits, sigmoid=False, dim=1):\n",
    "    if isinstance(logits, (list, tuple)):\n",
    "        logits = logits[0]\n",
    "\n",
    "    if sigmoid:\n",
    "        pred = torch.sigmoid(logits)\n",
    "        pred = (pred >= 0.5)\n",
    "    else:\n",
    "        pred = torch.softmax(logits, dim=dim)\n",
    "        pred = torch.argmax(pred, dim=dim, keepdim=True).to(dtype=torch.uint8)\n",
    "\n",
    "    return pred\n",
    "\n",
    "def _add_normalization_transforms(ts, key, normalize_mode, intensity_bounds):\n",
    "    if normalize_mode == \"none\":\n",
    "        pass\n",
    "    elif normalize_mode in [\"range\", \"ct\"]:\n",
    "        ts.append(ScaleIntensityRanged(keys=key, a_min=intensity_bounds[0], a_max=intensity_bounds[1],\n",
    "                                     b_min=-1, b_max=1, clip=False))\n",
    "        ts.append(Lambdad(keys=key, func=lambda x: torch.sigmoid(x)))\n",
    "    elif normalize_mode in [\"meanstd\", \"mri\"]:\n",
    "        ts.append(NormalizeIntensityd(keys=key, nonzero=True, channel_wise=True))\n",
    "    elif normalize_mode in [\"meanstdtanh\"]:\n",
    "        ts.append(NormalizeIntensityd(keys=key, nonzero=True, channel_wise=True))\n",
    "        ts.append(Lambdad(keys=key, func=lambda x: 3 * torch.tanh(x / 3)))\n",
    "    elif normalize_mode in [\"pet\"]:\n",
    "        ts.append(Lambdad(keys=key, func=lambda x: torch.sigmoid((x - x.min()) / x.std())))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported normalize_mode\" + str(normalize_mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference_verbose(\n",
    "    model_file=r\"J:\\startup\\whole-head-05mm-v1.0.1\\model.pt\",\n",
    "    image_file=r\"J:\\startup\\skull_ct_RAS00.nrrd\",\n",
    "    result_file=r\"J:\\startup\\output_seg01.nrrd\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2680f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: J:\\startup\\whole-head-05mm-v1.0.1\\model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "`apex.normalization.InstanceNorm3dNVFuser` is not installed properly, use nn.InstanceNorm3d instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Epoch 450, Best Metric 0.9060699939727783\n",
      "Moving model to device: cuda:0\n",
      "Mode: Standard\n",
      "Using crop_foreground\n",
      "Using resample with resample_resolution [0.45703124999999994, 0.459, 0.458015625]\n",
      "Running Inference ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "100%|██████████| 125/125 [13:22<00:00,  6.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 10, 512, 496, 338])\n",
      "Preds shape: torch.Size([1, 1, 512, 496, 338])\n",
      "Applying KeepLargestConnectedComponentd (whole-head model detected)\n",
      "Preds inverted shape: torch.Size([512, 512, 258])\n",
      "\n",
      "--- Computation Time Log ---\n",
      "  Loading volumes     : 5.63 seconds\n",
      "  Preprocessing       : 7.21 seconds\n",
      "  Inference           : 802.57 seconds\n",
      "  Logits              : 26.98 seconds\n",
      "  Preds               : 31.02 seconds\n",
      "  Convert to array    : 0.91 seconds\n",
      "  Save                : 2.60 seconds\n",
      "\n",
      "ALL DONE. Result saved in: J:\\startup\\output_seg02.nrrd\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def run_inference(model_file, image_file, result_file, save_mode=None, \n",
    "                 image_file_2=None, image_file_3=None, image_file_4=None):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    timing_checkpoints = []  # list of (operation, time) tuples\n",
    "\n",
    "    # --- Checking for model file ---\n",
    "    if not os.path.exists(model_file):\n",
    "        raise ValueError('Cannot find model file:' + str(model_file))\n",
    "\n",
    "    print(f\"Loading model from: {model_file}\")\n",
    "    checkpoint = torch.load(model_file, map_location=\"cpu\")\n",
    "\n",
    "    if 'config' not in checkpoint:\n",
    "        raise ValueError('Config not found in checkpoint (not a auto3dseg/segresnet model):' + str(model_file))\n",
    "\n",
    "    config = checkpoint[\"config\"]\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "    epoch = checkpoint.get(\"epoch\", 0)\n",
    "    best_metric = checkpoint.get(\"best_metric\", 0)\n",
    "    sigmoid = config.get(\"sigmoid\", False)\n",
    "\n",
    "    model = ConfigParser(config[\"network\"]).get_parsed_content()\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "    print(f'Model loaded: Epoch {epoch}, Best Metric {best_metric}')\n",
    "\n",
    "    # Select device based on previous check\n",
    "    device = torch.device(\"cpu\") if torch.cuda.device_count() == 0 else torch.device(0)\n",
    "    print(f\"Moving model to device: {device}\")\n",
    "    \n",
    "    model = model.to(device=device, memory_format=torch.channels_last_3d)\n",
    "    model.eval()\n",
    "\n",
    "    # --- BRATS Mode Check ---\n",
    "    if save_mode == 'brats' or 'brats' in model_file:\n",
    "        print(\"Mode: BRATS\")\n",
    "        image_files = []\n",
    "        for index, img in enumerate([image_file, image_file_2, image_file_3, image_file_4]):\n",
    "            if img is not None:\n",
    "                image_files.append(img)\n",
    "\n",
    "        for img in image_files:\n",
    "            if img is None or not os.path.exists(img):\n",
    "                raise ValueError(f'Incorrect image filename for {img}: \"{img}\"')\n",
    "\n",
    "        ts = [\n",
    "            LoadImaged(keys=\"image\", ensure_channel_first=True, dtype=None, allow_missing_keys=True, image_only=False),\n",
    "            EnsureTyped(keys=\"image\", data_type=\"tensor\", dtype=torch.float, allow_missing_keys=True)\n",
    "        ]\n",
    "\n",
    "        if config.get(\"orientation_ras\", False):\n",
    "            print('Using orientation_ras')\n",
    "            ts.append(Orientationd(keys=\"image\", axcodes=\"RAS\"))\n",
    "        \n",
    "        if config.get(\"crop_foreground\", True):\n",
    "            print('Using crop_foreground')\n",
    "            ts.append(CropForegroundd(keys=\"image\", source_key=\"image\", margin=10, allow_smaller=True))\n",
    "\n",
    "        if config.get(\"resample_resolution\", None) is not None:\n",
    "            pixdim = list(config[\"resample_resolution\"])\n",
    "            print(f'Using resample with resample_resolution {pixdim}')\n",
    "            ts.append(\n",
    "                Spacingd(\n",
    "                    keys=[\"image\"], pixdim=list(pixdim), mode=[\"bilinear\"], dtype=torch.float,\n",
    "                    min_pixdim=np.array(pixdim) * 0.75, max_pixdim=np.array(pixdim) * 1.25, allow_missing_keys=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        main_normalize_mode = config[\"normalize_mode\"]\n",
    "        intensity_bounds = config[\"intensity_bounds\"]\n",
    "        _add_normalization_transforms(ts, 'image', main_normalize_mode, intensity_bounds)\n",
    "\n",
    "        inf_transform = Compose(ts)\n",
    "        roi_size = config[\"roi_size\"]\n",
    "        sliding_inferrer = SlidingWindowInfererAdapt(roi_size=roi_size, sw_batch_size=1, overlap=0.625, mode=\"gaussian\",\n",
    "                                                     cache_roi_weight_map=False, progress=True)\n",
    "\n",
    "        batch_data = inf_transform([{\"image\": image_files}])\n",
    "        original_affine = batch_data[0]['image'].meta[MetaKeys.ORIGINAL_AFFINE]\n",
    "        batch_data = list_data_collate([batch_data])\n",
    "        data = batch_data[\"image\"].as_subclass(torch.Tensor).to(memory_format=torch.channels_last_3d, device=device)\n",
    "        timing_checkpoints.append((\"Preprocessing\", time.time()))\n",
    "\n",
    "        print('Running Inference ...')\n",
    "        with autocast(enabled=True):\n",
    "            logits = sliding_inferrer(inputs=data, network=model)\n",
    "        timing_checkpoints.append((\"Inference\", time.time()))\n",
    "\n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        \n",
    "        try:\n",
    "            pred = logits2pred(logits, sigmoid=sigmoid)\n",
    "        except RuntimeError as e:\n",
    "            if not logits.is_cuda:\n",
    "                raise e\n",
    "            print(f\"logits2pred failed on GPU, retrying on CPU. Shape: {logits.shape}\")\n",
    "            logits = logits.cpu()\n",
    "            pred = logits2pred(logits, sigmoid=sigmoid)\n",
    "        \n",
    "        print(f\"Preds shape: {pred.shape}\")\n",
    "        timing_checkpoints.append((\"Logits\", time.time()))\n",
    "        logits = None\n",
    "\n",
    "        post_transforms = Compose([Invertd(keys=\"pred\", orig_keys=\"image\", transform=inf_transform, nearest_interp=True)])\n",
    "\n",
    "        batch_data[\"pred\"] = convert_to_dst_type(pred, batch_data[\"image\"], dtype=pred.dtype, device=pred.device)[0]\n",
    "        pred = [post_transforms(x)[\"pred\"] for x in decollate_batch(batch_data)]\n",
    "        seg = pred[0]\n",
    "        print(f\"Preds inverted shape: {seg.shape}\")\n",
    "        timing_checkpoints.append((\"Preds\", time.time()))\n",
    "\n",
    "        # Merge BRATS channels\n",
    "        p2 = 2 * seg.any(0).to(dtype=torch.uint8)\n",
    "        p2[seg[1:].any(0)] = 1\n",
    "        p2[seg[2:].any(0)] = 3\n",
    "        seg = p2\n",
    "        print(f\"Updated seg for BRATS: {seg.shape}\")\n",
    "\n",
    "    # --- Standard Mode (Non-BRATS) ---\n",
    "    else:\n",
    "        print(\"Mode: Standard\")\n",
    "        image_files = {}\n",
    "        for index, img in enumerate([image_file, image_file_2, image_file_3, image_file_4]):\n",
    "            if img is not None:\n",
    "                image_files[f\"image{index + 1}\"] = img\n",
    "\n",
    "        keys = list(image_files.keys())\n",
    "        for img in image_files.keys():\n",
    "            if image_files[img] is None or not os.path.exists(image_files[img]):\n",
    "                raise ValueError(f'Incorrect image filename for {img}: \"{image_files[img]}\"')\n",
    "\n",
    "        loader = LoadImaged(keys=keys, ensure_channel_first=True, dtype=None, allow_missing_keys=True, image_only=False)\n",
    "        images_loaded = loader(image_files)\n",
    "        timing_checkpoints.append((\"Loading volumes\", time.time()))\n",
    "\n",
    "        if len(keys) > 1:\n",
    "            image1_shape = images_loaded[keys[0]].shape[1:]\n",
    "            for idx, img in enumerate(keys[1:]):\n",
    "                temp_shape = images_loaded[img].shape[-len(image1_shape):]\n",
    "                if np.any(np.not_equal(image1_shape, temp_shape)):\n",
    "                    print(f'Volumes do not have the same size - Resizing volume {img}')\n",
    "                    resizer = Resized(keys=img, spatial_size=image1_shape, mode='bilinear')\n",
    "                    images_loaded = resizer(images_loaded)\n",
    "                    timing_checkpoints.append((f\"Resizing volume {img}\", time.time()))\n",
    "\n",
    "        main_normalize_mode = config[\"normalize_mode\"]\n",
    "        intensity_bounds = config[\"intensity_bounds\"]\n",
    "        \n",
    "        if len(keys) == 1:\n",
    "            ts = [\n",
    "                ConcatItemsd(keys=keys, name=\"image\", dim=0),\n",
    "                EnsureTyped(keys=\"image\", data_type=\"tensor\", dtype=torch.float, allow_missing_keys=True)\n",
    "            ]\n",
    "            _add_normalization_transforms(ts, \"image\", main_normalize_mode, intensity_bounds)\n",
    "        else:\n",
    "            ts = []\n",
    "            extra_modalities = OrderedDict(config['extra_modalities'])\n",
    "            normalize_modes = [main_normalize_mode] + list(extra_modalities.values())\n",
    "            for key, normalize_mode in zip(keys, normalize_modes):\n",
    "                _add_normalization_transforms(ts, key, normalize_mode, intensity_bounds)\n",
    "            ts.extend([\n",
    "                ConcatItemsd(keys=keys, name=\"image\", dim=0),\n",
    "                EnsureTyped(keys=\"image\", data_type=\"tensor\", dtype=torch.float, allow_missing_keys=True)\n",
    "            ])\n",
    "\n",
    "        if config.get(\"orientation_ras\", False):\n",
    "            print('Using orientation_ras')\n",
    "            ts.append(Orientationd(keys=\"image\", axcodes=\"RAS\"))\n",
    "        \n",
    "        if config.get(\"crop_foreground\", True):\n",
    "            print('Using crop_foreground')\n",
    "            ts.append(CropForegroundd(keys=\"image\", source_key=\"image1\", margin=10, allow_smaller=True))\n",
    "\n",
    "        if config.get(\"resample_resolution\", None) is not None:\n",
    "            pixdim = list(config[\"resample_resolution\"])\n",
    "            print(f'Using resample with resample_resolution {pixdim}')\n",
    "            ts.append(\n",
    "                Spacingd(\n",
    "                    keys=[\"image\"], pixdim=list(pixdim), mode=[\"bilinear\"], dtype=torch.float,\n",
    "                    min_pixdim=np.array(pixdim) * 0.75, max_pixdim=np.array(pixdim) * 1.25, allow_missing_keys=True,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        inf_transform = Compose(ts)\n",
    "        roi_size = config[\"roi_size\"]\n",
    "        sliding_inferrer = SlidingWindowInfererAdapt(roi_size=roi_size, sw_batch_size=1, overlap=0.625, mode=\"gaussian\",\n",
    "                                                     cache_roi_weight_map=False, progress=True)\n",
    "\n",
    "        batch_data = inf_transform([images_loaded])\n",
    "        original_affine = batch_data[0]['image'].meta[MetaKeys.ORIGINAL_AFFINE]\n",
    "        batch_data = list_data_collate([batch_data])\n",
    "        data = batch_data[\"image\"].as_subclass(torch.Tensor).to(memory_format=torch.channels_last_3d, device=device)\n",
    "        timing_checkpoints.append((\"Preprocessing\", time.time()))\n",
    "\n",
    "        print('Running Inference ...')\n",
    "        with autocast(enabled=True):\n",
    "            logits = sliding_inferrer(inputs=data, network=model)\n",
    "        timing_checkpoints.append((\"Inference\", time.time()))\n",
    "\n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        try:\n",
    "            pred = logits2pred(logits, sigmoid=sigmoid)\n",
    "        except RuntimeError as e:\n",
    "            if not logits.is_cuda:\n",
    "                raise e\n",
    "            print(f\"logits2pred failed on GPU, retrying on CPU. Shape: {logits.shape}\")\n",
    "            logits = logits.cpu()\n",
    "            pred = logits2pred(logits, sigmoid=sigmoid)\n",
    "        \n",
    "        print(f\"Preds shape: {pred.shape}\")\n",
    "        timing_checkpoints.append((\"Logits\", time.time()))\n",
    "        logits = None\n",
    "\n",
    "        post_transforms_list = [Invertd(keys=\"pred\", orig_keys=\"image\", transform=inf_transform, nearest_interp=True)]\n",
    "        if 'whole-head' in model_file:\n",
    "            print(\"Applying KeepLargestConnectedComponentd (whole-head model detected)\")\n",
    "            post_transforms_list.append(KeepLargestConnectedComponentd(keys=\"pred\", num_components=2)) \n",
    "        \n",
    "        post_transforms = Compose(post_transforms_list)\n",
    "\n",
    "        batch_data[\"pred\"] = convert_to_dst_type(pred, batch_data[\"image\"], dtype=pred.dtype, device=pred.device)[0]\n",
    "        pred = [post_transforms(x)[\"pred\"] for x in decollate_batch(batch_data)]\n",
    "        seg = pred[0][0]\n",
    "\n",
    "    print(f\"Preds inverted shape: {seg.shape}\")\n",
    "    timing_checkpoints.append((\"Preds\", time.time()))\n",
    "\n",
    "    seg = seg.cpu().numpy().astype(np.uint8)\n",
    "    timing_checkpoints.append((\"Convert to array\", time.time()))\n",
    "\n",
    "    # Save result\n",
    "    nrrd_header = nrrd.read_header(image_file)\n",
    "    nrrd.write(result_file, seg, nrrd_header)\n",
    "    timing_checkpoints.append((\"Save\", time.time()))\n",
    "\n",
    "    print(\"\\n--- Computation Time Log ---\")\n",
    "    previous_start_time = start_time\n",
    "    for timing_checkpoint in timing_checkpoints:\n",
    "        print(f\"  {timing_checkpoint[0]:<20}: {timing_checkpoint[1] - previous_start_time:.2f} seconds\")\n",
    "        previous_start_time = timing_checkpoint[1]\n",
    "\n",
    "    print(f'\\nALL DONE. Result saved in: {result_file}')\n",
    "\n",
    "\n",
    "# Execute\n",
    "run_inference(\n",
    "    model_file=MODEL_FILE,\n",
    "    image_file=IMAGE_FILE,\n",
    "    result_file=RESULT_FILE,\n",
    "    image_file_2=IMAGE_FILE_2,\n",
    "    image_file_3=IMAGE_FILE_3,\n",
    "    image_file_4=IMAGE_FILE_4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd492da1",
   "metadata": {},
   "source": [
    "chatgpt code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67534ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import nrrd\n",
    "\n",
    "from collections import OrderedDict\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from monai.bundle import ConfigParser\n",
    "from monai.data import decollate_batch, list_data_collate\n",
    "from monai.inferers import SlidingWindowInfererAdapt\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureTyped,\n",
    "    ConcatItemsd,\n",
    "    Orientationd,\n",
    "    CropForegroundd,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    "    KeepLargestConnectedComponentd,\n",
    ")\n",
    "from monai.utils import MetaKeys, convert_to_dst_type\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper: normalization (same logic used in Auto3DSeg)\n",
    "# ------------------------------------------------------------\n",
    "def _add_normalization_transforms(ts, key, normalize_mode, intensity_bounds):\n",
    "    if normalize_mode == \"range\":\n",
    "        from monai.transforms import ScaleIntensityRanged\n",
    "        ts.append(\n",
    "            ScaleIntensityRanged(\n",
    "                keys=key,\n",
    "                a_min=intensity_bounds[0],\n",
    "                a_max=intensity_bounds[1],\n",
    "                b_min=0.0,\n",
    "                b_max=1.0,\n",
    "                clip=True,\n",
    "            )\n",
    "        )\n",
    "    elif normalize_mode == \"meanstd\":\n",
    "        from monai.transforms import NormalizeIntensityd\n",
    "        ts.append(NormalizeIntensityd(keys=key, nonzero=True, channel_wise=True))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Main inference\n",
    "# ------------------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def run_inference_verbose(\n",
    "    model_file,\n",
    "    image_file,\n",
    "    result_file,\n",
    "    save_mode=None,\n",
    "    image_file_2=None,\n",
    "    image_file_3=None,\n",
    "    image_file_4=None,\n",
    "):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # ------------------ DEVICE ------------------\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    print(f\"[DEVICE] {device}\")\n",
    "\n",
    "    # ------------------ LOAD MODEL ------------------\n",
    "    if not os.path.exists(model_file):\n",
    "        raise FileNotFoundError(model_file)\n",
    "\n",
    "    checkpoint = torch.load(model_file, map_location=\"cpu\")\n",
    "    config = checkpoint[\"config\"]\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "\n",
    "    model = ConfigParser(config[\"network\"]).get_parsed_content()\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model.to(device=device, memory_format=torch.channels_last_3d)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"[MODEL] Loaded (epoch={checkpoint.get('epoch', '?')}, \"\n",
    "          f\"best_metric={checkpoint.get('best_metric', '?')})\")\n",
    "\n",
    "    sigmoid = config.get(\"sigmoid\", False)\n",
    "\n",
    "    # ------------------ MODE ------------------\n",
    "    is_brats = save_mode == \"brats\" or (\n",
    "        save_mode is None and \"brats\" in model_file.lower()\n",
    "    )\n",
    "    print(f\"[MODE] {'BRATS' if is_brats else 'STANDARD'}\")\n",
    "\n",
    "    # ------------------ INPUT FILES ------------------\n",
    "    input_files = [image_file, image_file_2, image_file_3, image_file_4]\n",
    "    input_files = [f for f in input_files if f is not None]\n",
    "\n",
    "    for f in input_files:\n",
    "        if not os.path.exists(f):\n",
    "            raise FileNotFoundError(f)\n",
    "\n",
    "    # ------------------ LOAD ------------------\n",
    "    if is_brats:\n",
    "        data_dict = {\"image\": input_files}\n",
    "        keys = [\"image\"]\n",
    "    else:\n",
    "        data_dict = {f\"image{i+1}\": f for i, f in enumerate(input_files)}\n",
    "        keys = list(data_dict.keys())\n",
    "\n",
    "    loader = LoadImaged(keys=keys, ensure_channel_first=True, image_only=False)\n",
    "    loaded = loader(data_dict)\n",
    "\n",
    "    print(\"[DATA] Loaded volumes:\")\n",
    "    for k in keys:\n",
    "        print(f\"  {k}: {loaded[k].shape}\")\n",
    "\n",
    "    # ------------------ TRANSFORMS ------------------\n",
    "    ts = []\n",
    "\n",
    "    if not is_brats:\n",
    "        ts.append(ConcatItemsd(keys=keys, name=\"image\", dim=0))\n",
    "\n",
    "    ts.append(EnsureTyped(keys=\"image\", dtype=torch.float))\n",
    "\n",
    "    _add_normalization_transforms(\n",
    "        ts,\n",
    "        \"image\",\n",
    "        config[\"normalize_mode\"],\n",
    "        config[\"intensity_bounds\"],\n",
    "    )\n",
    "\n",
    "    if config.get(\"orientation_ras\", False):\n",
    "        ts.append(Orientationd(keys=\"image\", axcodes=\"RAS\"))\n",
    "\n",
    "    if config.get(\"crop_foreground\", True):\n",
    "        ts.append(CropForegroundd(keys=\"image\", source_key=\"image\"))\n",
    "\n",
    "    if config.get(\"resample_resolution\") is not None:\n",
    "        pixdim = list(config[\"resample_resolution\"])\n",
    "        ts.append(\n",
    "            Spacingd(\n",
    "                keys=\"image\",\n",
    "                pixdim=pixdim,\n",
    "                mode=\"bilinear\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    inf_transform = Compose(ts)\n",
    "\n",
    "    batch = inf_transform([loaded])\n",
    "    meta = batch[0][\"image\"].meta\n",
    "    batch = list_data_collate(batch)\n",
    "\n",
    "    data = batch[\"image\"].to(device, memory_format=torch.channels_last_3d)\n",
    "\n",
    "    print(f\"[DATA] Final tensor: {tuple(data.shape)}\")\n",
    "\n",
    "    # ------------------ INFERENCE ------------------\n",
    "    roi_size = config[\"roi_size\"]\n",
    "    inferer = SlidingWindowInfererAdapt(\n",
    "        roi_size=roi_size,\n",
    "        sw_batch_size=1,\n",
    "        overlap=0.625,\n",
    "        mode=\"gaussian\",\n",
    "        progress=True,\n",
    "    )\n",
    "\n",
    "    print(\"[RUN] Inference started\")\n",
    "    with autocast(enabled=use_cuda):\n",
    "        logits = inferer(data, model)\n",
    "\n",
    "    print(f\"[RUN] Logits shape: {tuple(logits.shape)}\")\n",
    "\n",
    "    # ------------------ PRED ------------------\n",
    "    from monai.networks.utils import one_hot\n",
    "    if sigmoid:\n",
    "        pred = (logits.sigmoid() > 0.5).float()\n",
    "    else:\n",
    "        pred = torch.argmax(logits, dim=1, keepdim=True)\n",
    "\n",
    "    logits = None\n",
    "\n",
    "    # ------------------ INVERT ------------------\n",
    "    post = [Invertd(keys=\"pred\", orig_keys=\"image\", transform=inf_transform)]\n",
    "    if \"whole-head\" in model_file.lower():\n",
    "        post.append(KeepLargestConnectedComponentd(keys=\"pred\"))\n",
    "\n",
    "    post = Compose(post)\n",
    "\n",
    "    batch[\"pred\"] = convert_to_dst_type(\n",
    "        pred, batch[\"image\"], device=pred.device\n",
    "    )[0]\n",
    "\n",
    "    seg = decollate_batch(batch)[0][\"pred\"]\n",
    "\n",
    "    print(f\"[POST] Segmentation shape: {tuple(seg.shape)}\")\n",
    "\n",
    "    # ------------------ SAVE NRRD (CORRECT METADATA) ------------------\n",
    "    seg_np = seg.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "    header = {\n",
    "        \"space\": meta.get(\"space\"),\n",
    "        \"space directions\": meta.get(\"space_directions\"),\n",
    "        \"space origin\": meta.get(\"space_origin\"),\n",
    "    }\n",
    "\n",
    "    nrrd.write(result_file, seg_np, header)\n",
    "    print(f\"[SAVE] {result_file}\")\n",
    "\n",
    "    print(f\"[DONE] Total time: {time.time() - t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "175644bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEVICE] cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MODEL] Loaded (epoch=450, best_metric=0.9060699939727783)\n",
      "[MODE] STANDARD\n",
      "[DATA] Loaded volumes:\n",
      "  image1: torch.Size([1, 512, 512, 258])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DATA] Final tensor: (1, 1, 525, 498, 338)\n",
      "[RUN] Inference started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [38:44<00:00, 18.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RUN] Logits shape: (1, 10, 525, 498, 338)\n",
      "[POST] Segmentation shape: (1, 525, 498, 338)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_inference_verbose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJ:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mstartup\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mwhole-head-05mm-v1.0.1\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmodel.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJ:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mstartup\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mskull_ct_RAS00.nrrd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJ:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mstartup\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43moutput_seg03.nrrd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\johnp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[5], line 204\u001b[0m, in \u001b[0;36mrun_inference_verbose\u001b[1;34m(model_file, image_file, result_file, save_mode, image_file_2, image_file_3, image_file_4)\u001b[0m\n\u001b[0;32m    196\u001b[0m seg_np \u001b[38;5;241m=\u001b[39m seg\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    198\u001b[0m header \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace\u001b[39m\u001b[38;5;124m\"\u001b[39m: meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace directions\u001b[39m\u001b[38;5;124m\"\u001b[39m: meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace_directions\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace origin\u001b[39m\u001b[38;5;124m\"\u001b[39m: meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspace_origin\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    202\u001b[0m }\n\u001b[1;32m--> 204\u001b[0m \u001b[43mnrrd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseg_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SAVE] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DONE] Total time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\johnp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nrrd\\writer.py:372\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(file, data, header, detached_header, relative_data_path, custom_field_map, compression_level, index_order)\u001b[0m\n\u001b[0;32m    369\u001b[0m     detached_header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[1;32m--> 372\u001b[0m     \u001b[43m_write_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_field_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;66;03m# If header & data in the same file is desired, write data in the file\u001b[39;00m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m detached_header:\n",
      "File \u001b[1;32mc:\\Users\\johnp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nrrd\\writer.py:177\u001b[0m, in \u001b[0;36m_write_header\u001b[1;34m(file, header, custom_field_map)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, (field, value) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ordered_options\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# Get the field_type based on field and then get corresponding\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# value as a str using _format_field_value\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     field_type \u001b[38;5;241m=\u001b[39m _get_field_type(field, custom_field_map)\n\u001b[1;32m--> 177\u001b[0m     value_str \u001b[38;5;241m=\u001b[39m \u001b[43m_format_field_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# Custom fields are written as key/value pairs with a := instead of : delimiter\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m custom_field_start_index:\n",
      "File \u001b[1;32mc:\\Users\\johnp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nrrd\\writer.py:99\u001b[0m, in \u001b[0;36m_format_field_value\u001b[1;34m(value, field_type)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m format_matrix(value)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m field_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdouble matrix\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformat_optional_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m field_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint vector list\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m format_optional_vector_list(value)\n",
      "File \u001b[1;32mc:\\Users\\johnp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nrrd\\formatters.py:143\u001b[0m, in \u001b[0;36mformat_optional_matrix\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Convert to float dtype to convert None to NaN\u001b[39;00m\n\u001b[0;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([format_optional_vector(y) \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x])\n",
      "\u001b[1;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "run_inference_verbose(\n",
    "    model_file=r\"J:\\startup\\whole-head-05mm-v1.0.1\\model.pt\",\n",
    "    image_file=r\"J:\\startup\\skull_ct_RAS00.nrrd\",\n",
    "    result_file=r\"J:\\startup\\output_seg03.nrrd\",\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daee5a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
